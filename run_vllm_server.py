#!/usr/bin/env python3
"""
vLLM ì„œë²„ ì‹¤í–‰ (ë¡œì»¬ ëª¨ë¸ ê³ ì •)
- í•­ìƒ /models/Qwen2.5-7B-Instruct ë§Œ ì‚¬ìš©
- 2ê°œ GPU(TP=2), í¬íŠ¸ 8001
"""

import os
import sys
import subprocess

MODEL_DIR = "/models/Qwen2.5-7B-Instruct"
HOST = "0.0.0.0"
PORT = "8001"
TP = "4"                 # tensor-parallel-size (GPU 2ì¥)
MAX_MODEL_LEN = "8192"
GPU_UTIL = "0.9"

def main():
    # ë°˜ë“œì‹œ ë¡œì»¬ ë””ë ‰í„°ë¦¬ë§Œ ì‚¬ìš©
    if not (os.path.isdir(MODEL_DIR) and os.path.isfile(os.path.join(MODEL_DIR, "config.json"))):
        print(f"âŒ ëª¨ë¸ ë””ë ‰í„°ë¦¬ í™•ì¸ ì‹¤íŒ¨: {MODEL_DIR}\n"
              f" - í´ë”ì™€ config.json ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        sys.exit(1)

    # ì‚¬ìš©í•  GPU ê³ ì • (í•„ìš” ì‹œ ë°”ê¾¸ì„¸ìš”: "0,1" / "0" ë“±)
    os.environ.setdefault("CUDA_VISIBLE_DEVICES", "0,1,2,3")
    
    # ì‹œê°„ëŒ€ ì„¤ì • (í•œêµ­ ì‹œê°„)
    os.environ["TZ"] = "Asia/Seoul"
    os.environ["PYTHONPATH"] = os.environ.get("PYTHONPATH", "") + ":/usr/share/zoneinfo"

    print("ğŸš€ vLLM ì„œë²„ ì‹œì‘ (ë¡œì»¬ ëª¨ë¸ ê³ ì •)")
    print(f"ğŸ“ MODEL_DIR = {MODEL_DIR}")
    print(f"ğŸ–¥ï¸  CUDA_VISIBLE_DEVICES = {os.environ['CUDA_VISIBLE_DEVICES']}")
    print(f"â° ì‹œê°„ëŒ€: {os.environ['TZ']}")
    print(f"ğŸŒ http://{HOST}:{PORT}")
    print("=" * 60)

    cmd = [
        "python3", "-m", "vllm.entrypoints.openai.api_server",
        "--model", MODEL_DIR,
        "--tokenizer", MODEL_DIR,           # í† í¬ë‚˜ì´ì €ë„ ê°™ì€ ê²½ë¡œë¡œ ê³ ì •
        "--served-model-name", "Qwen/Qwen2.5-7B-Instruct",
        "--host", HOST, "--port", PORT,
        "--tensor-parallel-size", TP,
        "--gpu-memory-utilization", GPU_UTIL,
        "--max-model-len", MAX_MODEL_LEN,
        "--trust-remote-code",
        "--dtype", "auto",
    ]

    try:
        subprocess.run(cmd, check=True)
    except subprocess.CalledProcessError as e:
        print(f"âŒ ì„œë²„ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
